#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üèóÔ∏è –ú–û–î–£–õ–¨ –ê–ù–ê–õ–ò–ó–ê –°–¢–†–£–ö–¢–£–†–´ –°–ê–ô–¢–ê
=================================
–ê–Ω–∞–ª–∏–∑ DOM —Å—Ç—Ä—É–∫—Ç—É—Ä—ã, —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤ –∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤–µ–±-—Å–∞–π—Ç–æ–≤

–ê–≤—Ç–æ—Ä: Senior Python Developer
–í–µ—Ä—Å–∏—è: 3.0.0
"""

import requests
from bs4 import BeautifulSoup, Comment
import re
from urllib.parse import urljoin, urlparse
from collections import Counter, defaultdict
import time

try:
    from fake_useragent import UserAgent
    UA_AVAILABLE = True
except ImportError:
    UA_AVAILABLE = False

class StructureAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≤–µ–±-—Å–∞–π—Ç–æ–≤"""
    
    def __init__(self):
        self.ua = UserAgent() if UA_AVAILABLE else None
        self.session = requests.Session()
        
    def get_headers(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤"""
        if self.ua:
            user_agent = self.ua.random
        else:
            user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        
        return {
            'User-Agent': user_agent,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ru,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
    
    def get_page_content(self, url: str) -> BeautifulSoup:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏ –ø–∞—Ä—Å–∏–Ω–≥ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            response = self.session.get(url, headers=self.get_headers(), timeout=30)
            response.raise_for_status()
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É
            if response.encoding.lower() in ['iso-8859-1', 'windows-1252']:
                response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.content, 'html.parser')
            return soup
            
        except Exception as e:
            raise Exception(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
    
    def analyze_basic_info(self, soup: BeautifulSoup) -> dict:
        """–ê–Ω–∞–ª–∏–∑ –±–∞–∑–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        title = soup.find('title')
        title_text = title.get_text().strip() if title else ""
        
        # –ú–µ—Ç–∞-–æ–ø–∏—Å–∞–Ω–∏–µ
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        meta_desc_text = meta_desc.get('content', '') if meta_desc else ""
        
        # –Ø–∑—ã–∫
        html_tag = soup.find('html')
        language = html_tag.get('lang', 'unknown') if html_tag else 'unknown'
        
        # –ö–æ–¥–∏—Ä–æ–≤–∫–∞
        meta_charset = soup.find('meta', attrs={'charset': True})
        if not meta_charset:
            meta_charset = soup.find('meta', attrs={'http-equiv': 'Content-Type'})
            
        charset = 'unknown'
        if meta_charset:
            if meta_charset.get('charset'):
                charset = meta_charset.get('charset')
            elif meta_charset.get('content'):
                content = meta_charset.get('content', '')
                match = re.search(r'charset=([^;]+)', content)
                if match:
                    charset = match.group(1)
        
        return {
            'title': title_text,
            'meta_description': meta_desc_text,
            'language': language,
            'charset': charset
        }
    
    def analyze_dom_structure(self, soup: BeautifulSoup) -> dict:
        """–ê–Ω–∞–ª–∏–∑ DOM —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        all_elements = soup.find_all()
        
        # –ü–æ–¥—Å—á–µ—Ç —Ç–µ–≥–æ–≤
        tags = [elem.name for elem in all_elements if elem.name]
        unique_tags = list(set(tags))
        tag_counts = Counter(tags)
        
        # –ü–æ–¥—Å—á–µ—Ç –∫–ª–∞—Å—Å–æ–≤ –∏ ID
        all_classes = []
        all_ids = []
        
        for elem in all_elements:
            if elem.get('class'):
                all_classes.extend(elem.get('class'))
            if elem.get('id'):
                all_ids.append(elem.get('id'))
        
        class_counts = Counter(all_classes)
        
        # –ê–Ω–∞–ª–∏–∑ –≥–ª—É–±–∏–Ω—ã –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç–∏
        max_depth = self.calculate_max_depth(soup)
        
        # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ HTML5 —Ç–µ–≥–∏
        semantic_tags = ['article', 'section', 'nav', 'header', 'footer', 'main', 'aside', 'figure']
        has_semantic = any(tag in unique_tags for tag in semantic_tags)
        
        return {
            'total_elements': len(all_elements),
            'unique_tags': sorted(unique_tags),
            'tag_distribution': dict(tag_counts.most_common(20)),
            'classes_count': len(set(all_classes)),
            'ids_count': len(set(all_ids)),
            'popular_classes': dict(class_counts.most_common(20)),
            'max_nesting_depth': max_depth,
            'has_semantic_html5': has_semantic,
            'semantic_tags_found': [tag for tag in semantic_tags if tag in unique_tags]
        }
    
    def calculate_max_depth(self, element, current_depth=0):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –≥–ª—É–±–∏–Ω—ã –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç–∏"""
        max_depth = current_depth
        
        for child in element.children:
            if hasattr(child, 'children'):
                child_depth = self.calculate_max_depth(child, current_depth + 1)
                max_depth = max(max_depth, child_depth)
        
        return max_depth
    
    def detect_content_types(self, soup: BeautifulSoup) -> dict:
        """–î–µ—Ç–µ–∫—Ü–∏—è —Ç–∏–ø–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        
        content_types = {}
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        patterns = {
            'products': {
                'selectors': [
                    '[class*="product"]', '[class*="item"]', '[class*="card"]',
                    '[data-product]', '[data-item]', '[itemtype*="Product"]',
                    '[class*="goods"]', '[class*="catalog"]'
                ],
                'keywords': ['price', 'buy', 'cart', 'product', 'item', '—Ç–æ–≤–∞—Ä', '—Ü–µ–Ω–∞']
            },
            'articles': {
                'selectors': [
                    'article', '[class*="article"]', '[class*="post"]',
                    '[class*="news"]', '[class*="blog"]', '[class*="content"]'
                ],
                'keywords': ['read', 'article', 'post', 'news', 'blog', '—Å—Ç–∞—Ç—å—è', '–Ω–æ–≤–æ—Å—Ç–∏']
            },
            'navigation': {
                'selectors': [
                    'nav', '[class*="nav"]', '[class*="menu"]',
                    '[class*="breadcrumb"]', '[role="navigation"]',
                    'ul.menu', 'ul.nav'
                ],
                'keywords': ['menu', 'navigation', 'nav', 'breadcrumb', '–º–µ–Ω—é', '–Ω–∞–≤–∏–≥–∞—Ü–∏—è']
            },
            'forms': {
                'selectors': [
                    'form', '[class*="form"]', '[class*="search"]',
                    '[class*="contact"]', '[class*="subscribe"]', 'input', 'textarea'
                ],
                'keywords': ['form', 'search', 'contact', 'subscribe', 'login', '—Ñ–æ—Ä–º–∞', '–ø–æ–∏—Å–∫']
            },
            'lists': {
                'selectors': [
                    '[class*="list"]', '[class*="grid"]', '[class*="catalog"]',
                    'ul', 'ol', '[class*="items"]', 'table'
                ],
                'keywords': ['list', 'grid', 'catalog', 'items', 'collection', '—Å–ø–∏—Å–æ–∫', '–∫–∞—Ç–∞–ª–æ–≥']
            },
            'media': {
                'selectors': [
                    'img', 'video', 'audio', '[class*="image"]',
                    '[class*="photo"]', '[class*="gallery"]', 'picture'
                ],
                'keywords': ['image', 'photo', 'gallery', 'video', 'media', '–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ', '—Ñ–æ—Ç–æ']
            },
            'text_content': {
                'selectors': [
                    'p', 'div', 'span', '[class*="text"]',
                    '[class*="description"]', '[class*="content"]'
                ],
                'keywords': ['text', 'description', 'content', 'paragraph', '—Ç–µ–∫—Å—Ç', '–æ–ø–∏—Å–∞–Ω–∏–µ']
            }
        }
        
        for content_type, config in patterns.items():
            elements_found = set()
            
            # –ü–æ–∏—Å–∫ –ø–æ CSS —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º
            for selector in config['selectors']:
                try:
                    found = soup.select(selector)
                    elements_found.update(found)
                except Exception:
                    continue
            
            # –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –≤ –∫–ª–∞—Å—Å–∞—Ö –∏ —Ç–µ–∫—Å—Ç–µ
            for keyword in config['keywords']:
                # –í –∫–ª–∞—Å—Å–∞—Ö
                class_elements = soup.find_all(attrs={'class': re.compile(keyword, re.I)})
                elements_found.update(class_elements)
                
                # –í ID
                id_elements = soup.find_all(attrs={'id': re.compile(keyword, re.I)})
                elements_found.update(id_elements)
            
            content_types[content_type] = len(elements_found)
        
        return content_types
    
    def analyze_links_and_forms(self, soup: BeautifulSoup, base_url: str) -> dict:
        """–ê–Ω–∞–ª–∏–∑ —Å—Å—ã–ª–æ–∫ –∏ —Ñ–æ—Ä–º"""
        
        # –ê–Ω–∞–ª–∏–∑ —Å—Å—ã–ª–æ–∫
        links = soup.find_all('a', href=True)
        internal_links = []
        external_links = []
        
        base_domain = urlparse(base_url).netloc
        
        for link in links:
            href = link.get('href', '')
            if not href or href.startswith('#') or href.startswith('javascript:'):
                continue
                
            full_url = urljoin(base_url, href)
            parsed_url = urlparse(full_url)
            
            if parsed_url.netloc == base_domain or not parsed_url.netloc:
                internal_links.append(full_url)
            else:
                external_links.append(full_url)
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        internal_links = list(set(internal_links))
        external_links = list(set(external_links))
        
        # –ê–Ω–∞–ª–∏–∑ —Ñ–æ—Ä–º
        forms = []
        for form in soup.find_all('form'):
            form_data = {
                'action': form.get('action', ''),
                'method': form.get('method', 'GET').upper(),
                'inputs_count': len(form.find_all(['input', 'select', 'textarea'])),
                'has_file_upload': bool(form.find('input', type='file')),
                'classes': form.get('class', []),
                'id': form.get('id', '')
            }
            
            # –ê–Ω–∞–ª–∏–∑ –ø–æ–ª–µ–π —Ñ–æ—Ä–º—ã
            inputs = []
            for input_elem in form.find_all(['input', 'select', 'textarea']):
                input_data = {
                    'tag': input_elem.name,
                    'type': input_elem.get('type', ''),
                    'name': input_elem.get('name', ''),
                    'placeholder': input_elem.get('placeholder', ''),
                    'required': input_elem.has_attr('required')
                }
                inputs.append(input_data)
            
            form_data['inputs'] = inputs
            forms.append(form_data)
        
        return {
            'internal_links': internal_links,
            'external_links': external_links,
            'forms': forms,
            'internal_links_count': len(internal_links),
            'external_links_count': len(external_links),
            'forms_count': len(forms)
        }
    
    def analyze_scripts_and_styles(self, soup: BeautifulSoup) -> dict:
        """–ê–Ω–∞–ª–∏–∑ —Å–∫—Ä–∏–ø—Ç–æ–≤ –∏ —Å—Ç–∏–ª–µ–π"""
        
        # –ê–Ω–∞–ª–∏–∑ JavaScript
        scripts = []
        for script in soup.find_all('script'):
            script_info = {
                'src': script.get('src', ''),
                'inline': bool(script.string and script.string.strip()),
                'type': script.get('type', 'text/javascript'),
                'async': script.has_attr('async'),
                'defer': script.has_attr('defer')
            }
            scripts.append(script_info)
        
        # –ê–Ω–∞–ª–∏–∑ CSS
        styles = []
        for link in soup.find_all('link', rel='stylesheet'):
            style_info = {
                'href': link.get('href', ''),
                'media': link.get('media', 'all'),
                'type': link.get('type', 'text/css')
            }
            styles.append(style_info)
        
        # Inline —Å—Ç–∏–ª–∏
        inline_styles = len(soup.find_all(attrs={'style': True}))
        
        # –ü–æ–∏—Å–∫ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–æ–≤
        page_content = str(soup).lower()
        frameworks = {
            'jquery': 'jquery' in page_content,
            'react': 'react' in page_content or '_react' in page_content,
            'vue': 'vue.js' in page_content or 'vue.min.js' in page_content,
            'angular': 'angular' in page_content,
            'bootstrap': 'bootstrap' in page_content,
            'foundation': 'foundation' in page_content
        }
        
        return {
            'scripts': scripts,
            'scripts_count': len(scripts),
            'external_scripts': len([s for s in scripts if s['src']]),
            'inline_scripts': len([s for s in scripts if s['inline']]),
            'styles': styles,
            'styles_count': len(styles),
            'inline_styles_count': inline_styles,
            'frameworks_detected': {k: v for k, v in frameworks.items() if v}
        }
    
    def generate_selectors(self, soup: BeautifulSoup, content_types: dict) -> dict:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤"""
        
        selectors = {}
        
        # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        content_patterns = {
            'products': ['[class*="product"]', '[class*="item"]', '[class*="card"]'],
            'articles': ['article', '[class*="article"]', '[class*="post"]'],
            'navigation': ['nav', '[class*="menu"]', '[class*="nav"]'],
            'forms': ['form', '[class*="form"]'],
            'lists': ['ul', 'ol', '[class*="list"]']
        }
        
        for content_type, patterns in content_patterns.items():
            type_selectors = []
            
            for pattern in patterns:
                try:
                    elements = soup.select(pattern)
                    if elements:
                        type_selectors.append({
                            'selector': pattern,
                            'count': len(elements),
                            'sample_text': elements[0].get_text().strip()[:100] if elements else ''
                        })
                except Exception:
                    continue
            
            if type_selectors:
                selectors[content_type] = sorted(type_selectors, key=lambda x: x['count'], reverse=True)
        
        # –û–±—â–∏–µ –ø–æ–ª–µ–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
        common_selectors = {
            'all_links': 'a[href]',
            'all_images': 'img[src]',
            'all_forms': 'form',
            'all_inputs': 'input, textarea, select',
            'headings': 'h1, h2, h3, h4, h5, h6',
            'paragraphs': 'p',
            'lists': 'ul, ol',
            'tables': 'table'
        }
        
        for name, selector in common_selectors.items():
            try:
                elements = soup.select(selector)
                if elements:
                    selectors[f'common_{name}'] = [{
                        'selector': selector,
                        'count': len(elements),
                        'description': f'–í—Å–µ {name.replace("_", " ")}'
                    }]
            except Exception:
                continue
        
        return selectors
    
    def analyze_performance_indicators(self, soup: BeautifulSoup) -> dict:
        """–ê–Ω–∞–ª–∏–∑ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        
        # –†–∞–∑–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        page_size = len(str(soup))
        
        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤
        images_count = len(soup.find_all('img'))
        scripts_count = len(soup.find_all('script'))
        styles_count = len(soup.find_all('link', rel='stylesheet'))
        
        # –í–Ω–µ—à–Ω–∏–µ —Ä–µ—Å—É—Ä—Å—ã
        external_resources = 0
        for elem in soup.find_all(['img', 'script', 'link']):
            src = elem.get('src') or elem.get('href', '')
            if src and (src.startswith('http') or src.startswith('//')):
                external_resources += 1
        
        # –û—Ü–µ–Ω–∫–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        complexity_score = (
            min(page_size / 1000, 100) +  # –†–∞–∑–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            min(images_count * 2, 50) +   # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            min(scripts_count * 3, 50) +  # –°–∫—Ä–∏–ø—Ç—ã
            min(external_resources, 50)   # –í–Ω–µ—à–Ω–∏–µ —Ä–µ—Å—É—Ä—Å—ã
        ) / 4
        
        return {
            'page_size_bytes': page_size,
            'images_count': images_count,
            'scripts_count': scripts_count,
            'styles_count': styles_count,
            'external_resources': external_resources,
            'complexity_score': round(complexity_score, 1),
            'loading_complexity': 'HIGH' if complexity_score > 70 else 'MEDIUM' if complexity_score > 40 else 'LOW'
        }
    
    def analyze(self, url: str) -> dict:
        """–ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
        
        results = {
            'url': url,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'analysis_type': 'structure'
        }
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            soup = self.get_page_content(url)
            
            # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            basic_info = self.analyze_basic_info(soup)
            results.update(basic_info)
            
            # DOM —Å—Ç—Ä—É–∫—Ç—É—Ä–∞
            dom_structure = self.analyze_dom_structure(soup)
            results.update(dom_structure)
            
            # –¢–∏–ø—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            content_types = self.detect_content_types(soup)
            results['content_types'] = content_types
            
            # –°—Å—ã–ª–∫–∏ –∏ —Ñ–æ—Ä–º—ã
            links_forms = self.analyze_links_and_forms(soup, url)
            results.update(links_forms)
            
            # –°–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
            scripts_styles = self.analyze_scripts_and_styles(soup)
            results['scripts'] = scripts_styles['scripts']
            results['styles'] = scripts_styles['styles']
            results['frameworks_detected'] = scripts_styles['frameworks_detected']
            
            # –°–µ–ª–µ–∫—Ç–æ—Ä—ã
            selectors = self.generate_selectors(soup, content_types)
            results['suggested_selectors'] = selectors
            
            # –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            performance = self.analyze_performance_indicators(soup)
            results['performance'] = performance
            
            # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞
            parsing_complexity = self.evaluate_parsing_complexity(results)
            results['parsing_complexity'] = parsing_complexity
            
            return results
            
        except Exception as e:
            results['error'] = str(e)
            results['success'] = False
            return results
    
    def evaluate_parsing_complexity(self, results: dict) -> dict:
        """–û—Ü–µ–Ω–∫–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        
        complexity_factors = []
        score = 0
        
        # JavaScript —Å–ª–æ–∂–Ω–æ—Å—Ç—å
        js_count = len(results.get('scripts', []))
        if js_count > 10:
            complexity_factors.append("–ú–Ω–æ–≥–æ JavaScript —Ñ–∞–π–ª–æ–≤")
            score += 30
        elif js_count > 5:
            score += 15
        
        # –§—Ä–µ–π–º–≤–æ—Ä–∫–∏
        frameworks = results.get('frameworks_detected', {})
        if frameworks:
            complexity_factors.append(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏: {', '.join(frameworks.keys())}")
            score += 25
        
        # –§–æ—Ä–º—ã
        forms_count = results.get('forms_count', 0)
        if forms_count > 3:
            complexity_factors.append("–ú–Ω–æ–≥–æ —Ñ–æ—Ä–º - –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è")
            score += 15
        
        # –í–Ω–µ—à–Ω–∏–µ —Ä–µ—Å—É—Ä—Å—ã
        performance = results.get('performance', {})
        if performance.get('loading_complexity') == 'HIGH':
            complexity_factors.append("–í—ã—Å–æ–∫–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å –∑–∞–≥—Ä—É–∑–∫–∏")
            score += 20
        
        # –ì–ª—É–±–∏–Ω–∞ –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç–∏
        if results.get('max_nesting_depth', 0) > 15:
            complexity_factors.append("–ì–ª—É–±–æ–∫–∞—è –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç—å DOM")
            score += 10
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Ä–æ–≤–Ω—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        if score >= 70:
            level = 'VERY_HIGH'
            recommendation = 'Selenium –∏–ª–∏ Playwright –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã'
        elif score >= 50:
            level = 'HIGH'
            recommendation = '–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è Selenium'
        elif score >= 30:
            level = 'MEDIUM'
            recommendation = 'Requests + BeautifulSoup —Å –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç—å—é'
        else:
            level = 'LOW'
            recommendation = '–ü—Ä–æ—Å—Ç–æ–π requests + BeautifulSoup'
        
        return {
            'score': score,
            'level': level,
            'factors': complexity_factors,
            'recommendation': recommendation,
            'estimated_effort': {
                'VERY_HIGH': '–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∏–µ',
                'HIGH': '–í—ã—Å–æ–∫–∏–µ', 
                'MEDIUM': '–°—Ä–µ–¥–Ω–∏–µ',
                'LOW': '–ù–∏–∑–∫–∏–µ'
            }.get(level, '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
        }
    
    def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        if hasattr(self.session, 'close'):
            self.session.close()